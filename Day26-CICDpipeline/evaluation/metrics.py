"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""
from typing import List, Dict, Any


def calculate_signal_detection_accuracy(
    detected_signals: List[Dict],
    ground_truth_signals: List[Dict]
) -> Dict[str, float]:
    """
    è®¡ç®—ä¿¡å·æ£€æµ‹å‡†ç¡®ç‡
    
    Args:
        detected_signals: ç³»ç»Ÿæ£€æµ‹åˆ°çš„å¼‚å¸¸ä¿¡å·åˆ—è¡¨
        ground_truth_signals: çœŸå®å¼‚å¸¸ä¿¡å·åˆ—è¡¨ï¼ˆground truthï¼‰
    
    Returns:
        åŒ…å«å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1çš„å­—å…¸
    """
    if not ground_truth_signals:
        return {
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "detected_count": len(detected_signals),
            "ground_truth_count": 0
        }
    
    # ç®€å•çš„åŒ¹é…é€»è¾‘ï¼šå¦‚æœæ£€æµ‹åˆ°çš„ä¿¡å·æ—¶é—´çª—å£ä¸ground truthé‡å 
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
    true_positives = 0
    false_positives = 0
    
    for detected in detected_signals:
        detected_start = detected.get('window_start')
        # æ£€æŸ¥æ˜¯å¦ä¸ä»»ä½•ground truthä¿¡å·åŒ¹é…
        matched = False
        for gt in ground_truth_signals:
            gt_start = gt.get('window_start')
            # ç®€å•çš„æ—¶é—´çª—å£åŒ¹é…ï¼ˆ5åˆ†é’Ÿçª—å£å†…ï¼‰
            # å®é™…åº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´åŒ¹é…é€»è¾‘
            if detected_start and gt_start:
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ—¶é—´æ ¼å¼è§£æå’Œæ¯”è¾ƒ
                matched = True
                break
        if matched:
            true_positives += 1
        else:
            false_positives += 1
    
    false_negatives = len(ground_truth_signals) - true_positives
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1": round(f1, 4),
        "true_positives": true_positives,
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "detected_count": len(detected_signals),
        "ground_truth_count": len(ground_truth_signals)
    }


def calculate_noise_reduction_rate(
    total_logs: int,
    candidate_logs: int
) -> Dict[str, Any]:
    """
    è®¡ç®—å™ªå£°å‡å°‘ç‡
    
    Args:
        total_logs: æ€»æ—¥å¿—æ•°
        candidate_logs: å€™é€‰æ—¥å¿—æ•°ï¼ˆä¿¡å·æ£€æµ‹åï¼‰
    
    Returns:
        å™ªå£°å‡å°‘ç‡ç»Ÿè®¡
    """
    if total_logs == 0:
        return {
            "noise_reduction_rate": 0.0,
            "total_logs": 0,
            "candidate_logs": 0,
            "filtered_logs": 0
        }
    
    filtered_logs = total_logs - candidate_logs
    reduction_rate = (filtered_logs / total_logs) * 100
    
    return {
        "noise_reduction_rate": round(reduction_rate, 2),
        "total_logs": total_logs,
        "candidate_logs": candidate_logs,
        "filtered_logs": filtered_logs,
        "reduction_ratio": f"{candidate_logs}/{total_logs}"
    }


def calculate_cost_efficiency(
    total_logs: int,
    candidate_logs: int,
    llm_tokens_used: int,
    baseline_tokens_used: int = None
) -> Dict[str, Any]:
    """
    è®¡ç®—æˆæœ¬æ•ˆç‡
    
    Args:
        total_logs: æ€»æ—¥å¿—æ•°
        candidate_logs: å€™é€‰æ—¥å¿—æ•°
        llm_tokens_used: LLMä½¿ç”¨çš„tokenæ•°
        baseline_tokens_used: Baselineæ–¹æ³•ä½¿ç”¨çš„tokenæ•°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æˆæœ¬æ•ˆç‡ç»Ÿè®¡
    """
    # ä¼°ç®—ï¼šå¦‚æœç›´æ¥å¤„ç†æ‰€æœ‰æ—¥å¿—éœ€è¦å¤šå°‘token
    # å‡è®¾æ¯æ¡æ—¥å¿—å¹³å‡50 tokens
    estimated_baseline_tokens = baseline_tokens_used if baseline_tokens_used else (total_logs * 50)
    
    cost_reduction = ((estimated_baseline_tokens - llm_tokens_used) / estimated_baseline_tokens) * 100 if estimated_baseline_tokens > 0 else 0.0
    
    return {
        "llm_tokens_used": llm_tokens_used,
        "estimated_baseline_tokens": estimated_baseline_tokens,
        "cost_reduction_percent": round(cost_reduction, 2),
        "tokens_per_log": round(llm_tokens_used / candidate_logs, 2) if candidate_logs > 0 else 0,
        "efficiency_ratio": f"{llm_tokens_used}/{estimated_baseline_tokens}"
    }


def calculate_root_cause_accuracy(
    predicted_root_cause: str,
    ground_truth_root_cause: str,
    use_semantic_similarity: bool = True
) -> Dict[str, Any]:
    """
    è®¡ç®—æ ¹å› è¯Šæ–­å‡†ç¡®ç‡
    
    Args:
        predicted_root_cause: ç³»ç»Ÿé¢„æµ‹çš„æ ¹å› 
        ground_truth_root_cause: çœŸå®æ ¹å› 
        use_semantic_similarity: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆéœ€è¦embeddingæ¨¡å‹ï¼‰
    
    Returns:
        å‡†ç¡®ç‡ç»Ÿè®¡
    """
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    predicted_words = set(predicted_root_cause.lower().split())
    ground_truth_words = set(ground_truth_root_cause.lower().split())
    
    if not ground_truth_words:
        return {
            "exact_match": False,
            "keyword_overlap": 0.0,
            "jaccard_similarity": 0.0
        }
    
    # Jaccardç›¸ä¼¼åº¦
    intersection = predicted_words & ground_truth_words
    union = predicted_words | ground_truth_words
    jaccard = len(intersection) / len(union) if union else 0.0
    
    # å…³é”®è¯é‡å ç‡
    keyword_overlap = len(intersection) / len(ground_truth_words) if ground_truth_words else 0.0
    
    # ç²¾ç¡®åŒ¹é…
    exact_match = predicted_root_cause.lower().strip() == ground_truth_root_cause.lower().strip()
    
    return {
        "exact_match": exact_match,
        "keyword_overlap": round(keyword_overlap, 4),
        "jaccard_similarity": round(jaccard, 4),
        "predicted": predicted_root_cause,
        "ground_truth": ground_truth_root_cause
    }


def calculate_processing_time_metrics(
    signal_detection_time: float,
    candidate_retrieval_time: float,
    rag_diagnosis_time: float,
    total_time: float
) -> Dict[str, Any]:
    """
    è®¡ç®—å¤„ç†æ—¶é—´æŒ‡æ ‡
    
    Args:
        signal_detection_time: ä¿¡å·æ£€æµ‹æ—¶é—´ï¼ˆç§’ï¼‰
        candidate_retrieval_time: å€™é€‰æ£€ç´¢æ—¶é—´ï¼ˆç§’ï¼‰
        rag_diagnosis_time: RAGè¯Šæ–­æ—¶é—´ï¼ˆç§’ï¼‰
        total_time: æ€»æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        æ—¶é—´ç»Ÿè®¡
    """
    return {
        "signal_detection_time": round(signal_detection_time, 3),
        "candidate_retrieval_time": round(candidate_retrieval_time, 3),
        "rag_diagnosis_time": round(rag_diagnosis_time, 3),
        "total_time": round(total_time, 3),
        "signal_detection_percent": round((signal_detection_time / total_time) * 100, 2) if total_time > 0 else 0,
        "candidate_retrieval_percent": round((candidate_retrieval_time / total_time) * 100, 2) if total_time > 0 else 0,
        "rag_diagnosis_percent": round((rag_diagnosis_time / total_time) * 100, 2) if total_time > 0 else 0
    }


def generate_evaluation_summary(metrics: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆè¯„ä¼°æ‘˜è¦
    
    Args:
        metrics: æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    
    Returns:
        æ ¼å¼åŒ–çš„æ‘˜è¦å­—ç¬¦ä¸²
    """
    summary_lines = [
        "=" * 60,
        "EVALUATION SUMMARY",
        "=" * 60
    ]
    
    if "signal_detection" in metrics:
        sd = metrics["signal_detection"]
        summary_lines.append("\nğŸ“Š Signal Detection:")
        summary_lines.append(f"  - Precision: {sd.get('precision', 0):.2%}")
        summary_lines.append(f"  - Recall: {sd.get('recall', 0):.2%}")
        summary_lines.append(f"  - F1-Score: {sd.get('f1', 0):.2%}")
    
    if "noise_reduction" in metrics:
        nr = metrics["noise_reduction"]
        summary_lines.append("\nğŸ”‡ Noise Reduction:")
        summary_lines.append(f"  - Reduction Rate: {nr.get('noise_reduction_rate', 0):.2f}%")
        summary_lines.append(f"  - Filtered: {nr.get('filtered_logs', 0)}/{nr.get('total_logs', 0)} logs")
    
    if "cost_efficiency" in metrics:
        ce = metrics["cost_efficiency"]
        summary_lines.append("\nğŸ’° Cost Efficiency:")
        summary_lines.append(f"  - Cost Reduction: {ce.get('cost_reduction_percent', 0):.2f}%")
        summary_lines.append(f"  - Tokens Used: {ce.get('llm_tokens_used', 0)}")
    
    if "root_cause" in metrics:
        rc = metrics["root_cause"]
        summary_lines.append("\nğŸ¯ Root Cause Diagnosis:")
        summary_lines.append(f"  - Keyword Overlap: {rc.get('keyword_overlap', 0):.2%}")
        summary_lines.append(f"  - Jaccard Similarity: {rc.get('jaccard_similarity', 0):.2%}")
    
    if "processing_time" in metrics:
        pt = metrics["processing_time"]
        summary_lines.append("\nâ±ï¸  Processing Time:")
        summary_lines.append(f"  - Total: {pt.get('total_time', 0):.3f}s")
        summary_lines.append(f"  - Signal Detection: {pt.get('signal_detection_time', 0):.3f}s ({pt.get('signal_detection_percent', 0):.1f}%)")
        summary_lines.append(f"  - RAG Diagnosis: {pt.get('rag_diagnosis_time', 0):.3f}s ({pt.get('rag_diagnosis_percent', 0):.1f}%)")
    
    summary_lines.append("\n" + "=" * 60)
    
    return "\n".join(summary_lines)
